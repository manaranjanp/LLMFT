{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"lxbzibRekIC3"},"outputs":[],"source":["!pip install -U transformers datasets"]},{"cell_type":"markdown","metadata":{"id":"NmDe28RukIC7"},"source":["# Summary of the tasks"]},{"cell_type":"markdown","metadata":{"id":"oTvh3DkDkIC9"},"source":["This page shows the most frequent use-cases when using the library. The models available allow for many different\n","configurations and a great versatility in use-cases. The most simple ones are presented here, showcasing usage for\n","tasks such as image classification, question answering, sequence classification, named entity recognition and others.\n","\n","These examples leverage auto-models, which are classes that will instantiate a model according to a given checkpoint,\n","automatically selecting the correct model architecture. Please check the [AutoModel](https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.AutoModel) documentation\n","for more information. Feel free to modify the code to be more specific and adapt it to your specific use-case.\n","\n","In order to do an inference on a task, several mechanisms are made available by the library:\n","\n","- Pipelines: very easy-to-use abstractions, which require as little as two lines of code.\n","- Direct model use: Less abstractions, but more flexibility and power via a direct access to a tokenizer\n","  (PyTorch/TensorFlow) and full inference capacity.\n","\n","Both approaches are showcased here.\n","\n","<Tip>\n","\n","All tasks presented here leverage pre-trained checkpoints that were fine-tuned on specific tasks. Loading a\n","checkpoint that was not fine-tuned on a specific task would load only the base transformer layers and not the\n","additional head that is used for the task, initializing the weights of that head randomly.\n","\n","This would produce random output.\n","\n","</Tip>"]},{"cell_type":"markdown","metadata":{"id":"VbhDnmwakIC-"},"source":["## Sequence Classification"]},{"cell_type":"markdown","metadata":{"id":"NAmG2aMTkIC_"},"source":["Sequence classification is the task of classifying sequences according to a given number of classes. An example of\n","sequence classification is the GLUE dataset, which is entirely based on that task.\n","\n","Here is an example of using pipelines to do sentiment analysis: identifying if a sequence is positive or negative. It\n","leverages a fine-tuned model on sst2, which is a GLUE task.\n","\n","This returns a label (\"POSITIVE\" or \"NEGATIVE\") alongside a score, as follows:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mco854-YkIDA"},"outputs":[],"source":["from transformers import pipeline\n","\n","classifier = pipeline(\"sentiment-analysis\")"]},{"cell_type":"code","source":["result = classifier(\"I hate you\")[0]\n","print(f\"label: {result['label']}, with score: {round(result['score'], 4)}\")"],"metadata":{"id":"VUncs6Y4CWTx"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JrxJn28ikIDC"},"outputs":[],"source":["result = classifier(\"I love you\")[0]\n","print(f\"label: {result['label']}, with score: {round(result['score'], 4)}\")"]},{"cell_type":"markdown","metadata":{"id":"5aiqOSh5kIDD"},"source":["Here is an example of doing a sequence classification using a model to determine if two sequences are paraphrases of\n","each other. The process is the following:\n","\n","1. Instantiate a tokenizer and a model from the checkpoint name. The model is identified as a BERT model and loads it\n","   with the weights stored in the checkpoint.\n","2. Build a sequence from the two sentences, with the correct model-specific separators, token type ids and attention\n","   masks (which will be created automatically by the tokenizer).\n","3. Pass this sequence through the model so that it is classified in one of the two available classes: 0 (not a\n","   paraphrase) and 1 (is a paraphrase).\n","4. Compute the softmax of the result to get probabilities over the classes.\n","5. Print the results."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"73kxEjJBkIDE"},"outputs":[],"source":["from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","import torch\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased-finetuned-sst-2-english\")\n","model = AutoModelForSequenceClassification.from_pretrained(\"distilbert/distilbert-base-uncased-finetuned-sst-2-english\")"]},{"cell_type":"code","source":["inputs = tokenizer(\"I really loved this movie!\", return_tensors=\"pt\")\n","\n","with torch.no_grad():\n","    logits = model(**inputs).logits\n","\n","predicted_class_id = logits.argmax().item()\n","model.config.id2label[predicted_class_id]"],"metadata":{"id":"8DEhAP1cC_qQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tMUq1Q95kIDG"},"source":["## Extractive Question Answering"]},{"cell_type":"markdown","metadata":{"id":"j6l8GjNLkIDG"},"source":["Extractive Question Answering is the task of extracting an answer from a text given a question. An example of a\n","question answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tune a\n","model on a SQuAD task, you may leverage the [run_qa.py](https://github.com/huggingface/transformers/tree/main/examples/pytorch/question-answering/run_qa.py) and\n","[run_tf_squad.py](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/question-answering/run_tf_squad.py)\n","scripts.\n","\n","\n","Here is an example of using pipelines to do question answering: extracting an answer from a text given a question. It\n","leverages a fine-tuned model on SQuAD."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R-AYSR5okIDG"},"outputs":[],"source":["from transformers import pipeline\n","\n","question_answerer = pipeline(\"question-answering\")\n","\n","context = r\"\"\"\n","Extractive Question Answering is the task of extracting an answer from a text given a question. An example of a\n","question answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tune\n","a model on a SQuAD task, you may leverage the examples/pytorch/question-answering/run_squad.py script.\n","\"\"\""]},{"cell_type":"markdown","metadata":{"id":"ZkNMCI4IkIDH"},"source":["This returns an answer extracted from the text, a confidence score, alongside \"start\" and \"end\" values, which are the\n","positions of the extracted answer in the text."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z94tZdflkIDI"},"outputs":[],"source":["result = question_answerer(question=\"What is extractive question answering?\", context=context)\n","print(\n","    f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\"\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Uk0qMuRzkIDJ"},"outputs":[],"source":["result = question_answerer(question=\"What is a good example of a question answering dataset?\", context=context)\n","print(\n","    f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\"\n",")"]},{"cell_type":"markdown","metadata":{"id":"TL17Jq3ukIDK"},"source":["Here is an example of question answering using a model and a tokenizer. The process is the following:\n","\n","1. Instantiate a tokenizer and a model from the checkpoint name. The model is identified as a BERT model and loads it\n","   with the weights stored in the checkpoint.\n","2. Define a text and a few questions.\n","3. Iterate over the questions and build a sequence from the text and the current question, with the correct\n","   model-specific separators, token type ids and attention masks.\n","4. Pass this sequence through the model. This outputs a range of scores across the entire sequence tokens (question and\n","   text), for both the start and end positions.\n","5. Compute the softmax of the result to get probabilities over the tokens.\n","6. Fetch the tokens from the identified start and stop values, convert those tokens to a string.\n","7. Print the results."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ARETK0RfkIDK"},"outputs":[],"source":["from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n","import torch\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n","model = AutoModelForQuestionAnswering.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n","\n","text = r\"\"\"\n","ü§ó Transformers (formerly known as pytorch-transformers and pytorch-pretrained-bert) provides general-purpose\n","architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet‚Ä¶) for Natural Language Understanding (NLU) and Natural\n","Language Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability between\n","TensorFlow 2.0 and PyTorch.\n","\"\"\"\n","\n","questions = [\n","    \"How many pretrained models are available in ü§ó Transformers?\",\n","    \"What does ü§ó Transformers provide?\",\n","    \"ü§ó Transformers provides interoperability between which frameworks?\",\n","]\n","\n","for question in questions:\n","    inputs = tokenizer(question, text, add_special_tokens=True, return_tensors=\"pt\")\n","    input_ids = inputs[\"input_ids\"].tolist()[0]\n","\n","    outputs = model(**inputs)\n","    answer_start_scores = outputs.start_logits\n","    answer_end_scores = outputs.end_logits\n","\n","    # Get the most likely beginning of answer with the argmax of the score\n","    answer_start = torch.argmax(answer_start_scores)\n","    # Get the most likely end of answer with the argmax of the score\n","    answer_end = torch.argmax(answer_end_scores) + 1\n","\n","    answer = tokenizer.convert_tokens_to_string(\n","        tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end])\n","    )\n","\n","    print(f\"Question: {question}\")\n","    print(f\"Answer: {answer}\")"]},{"cell_type":"markdown","metadata":{"id":"nQ-eiOZXkIDL"},"source":["## Language Modeling"]},{"cell_type":"markdown","metadata":{"id":"vFIbhWL5kIDL"},"source":["Language modeling is the task of fitting a model to a corpus, which can be domain specific. All popular\n","transformer-based models are trained using a variant of language modeling, e.g. BERT with masked language modeling,\n","GPT-2 with causal language modeling.\n","\n","Language modeling can be useful outside of pretraining as well, for example to shift the model distribution to be\n","domain-specific: using a language model trained over a very large corpus, and then fine-tuning it to a news dataset or\n","on scientific papers e.g. [LysandreJik/arxiv-nlp](https://huggingface.co/lysandre/arxiv-nlp)."]},{"cell_type":"markdown","metadata":{"id":"ahcghxXMkIDL"},"source":["### Masked Language Modeling"]},{"cell_type":"markdown","metadata":{"id":"QWgCHH_hkIDL"},"source":["Masked language modeling is the task of masking tokens in a sequence with a masking token, and prompting the model to\n","fill that mask with an appropriate token. This allows the model to attend to both the right context (tokens on the\n","right of the mask) and the left context (tokens on the left of the mask). Such a training creates a strong basis for\n","downstream tasks requiring bi-directional context, such as SQuAD (question answering, see [Lewis, Lui, Goyal et al.](https://arxiv.org/abs/1910.13461), part 4.2). If you would like to fine-tune a model on a masked language modeling\n","task, you may leverage the [run_mlm.py](https://github.com/huggingface/transformers/tree/main/examples/pytorch/language-modeling/run_mlm.py) script.\n","\n","Here is an example of using pipelines to replace a mask from a sequence:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UGozKHsykIDM"},"outputs":[],"source":["from transformers import pipeline\n","\n","unmasker = pipeline(\"fill-mask\")"]},{"cell_type":"markdown","metadata":{"id":"mdBw1qN8kIDM"},"source":["This outputs the sequences with the mask filled, the confidence score, and the token id in the tokenizer vocabulary:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cNPUw711kIDM"},"outputs":[],"source":["from pprint import pprint\n","\n","pprint(\n","    unmasker(\n","        f\"HuggingFace is creating a {unmasker.tokenizer.mask_token} that the community uses to solve NLP tasks.\"\n","    )\n",")"]},{"cell_type":"markdown","metadata":{"id":"39CBo2ogkIDM"},"source":["Here is an example of doing masked language modeling using a model and a tokenizer. The process is the following:\n","\n","1. Instantiate a tokenizer and a model from the checkpoint name. The model is identified as a DistilBERT model and\n","   loads it with the weights stored in the checkpoint.\n","2. Define a sequence with a masked token, placing the `tokenizer.mask_token` instead of a word.\n","3. Encode that sequence into a list of IDs and find the position of the masked token in that list.\n","4. Retrieve the predictions at the index of the mask token: this tensor has the same size as the vocabulary, and the\n","   values are the scores attributed to each token. The model gives higher score to tokens it deems probable in that\n","   context.\n","5. Retrieve the top 5 tokens using the PyTorch `topk` or TensorFlow `top_k` methods.\n","6. Replace the mask token by the tokens and print the results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"we5QIw_MkIDM"},"outputs":[],"source":["from transformers import AutoModelForMaskedLM, AutoTokenizer\n","import torch\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-cased\")\n","model = AutoModelForMaskedLM.from_pretrained(\"distilbert-base-cased\")"]},{"cell_type":"code","source":["sequence = (\n","    f\"I love drinking {tokenizer.mask_token} in the morning!\"\n",")\n","\n","# Tokenize input\n","inputs = tokenizer(sequence, return_tensors=\"pt\")\n","mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\n","\n","# Get model predictions\n","with torch.no_grad():\n","    outputs = model(**inputs)\n","    logits = outputs.logits\n","\n","# Get logits for the masked token\n","mask_token_logits = logits[0, mask_token_index, :]\n","\n","# Get top 5 predicted tokens\n","top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n","\n","# Print completed sentences\n","for token in top_5_tokens:\n","    predicted_token = tokenizer.decode([token])\n","    print(sequence.replace(tokenizer.mask_token, predicted_token))"],"metadata":{"id":"WDeWWBIDDkRg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YetKynoJkIDO"},"source":["### Text Generation (Causal Language Models)"]},{"cell_type":"markdown","metadata":{"id":"ShclGUGlkIDW"},"source":["In text generation (*a.k.a* *open-ended text generation*) the goal is to create a coherent portion of text that is a\n","continuation from the given context. The following example shows how *GPT-2* can be used in pipelines to generate text.\n","As a default all models apply *Top-K* sampling when used in pipelines, as configured in their respective configurations\n","(see [gpt-2 config](https://huggingface.co/gpt2/blob/main/config.json) for example)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jke2ii-lkIDW"},"outputs":[],"source":["from transformers import pipeline\n","\n","text_generator = pipeline(\"text-generation\")\n","print(text_generator(\"As far as I am concerned, I will\", max_length=50, do_sample=False))"]},{"cell_type":"markdown","metadata":{"id":"pi6s3QfBkIDX"},"source":["Here, the model generates a random text with a total maximal length of *50* tokens from context *\"As far as I am\n","concerned, I will\"*. Behind the scenes, the pipeline object calls the method\n","[PreTrainedModel.generate()](https://huggingface.co/docs/transformers/main/en/main_classes/text_generation#transformers.GenerationMixin.generate) to generate text. The default arguments for this method can be\n","overridden in the pipeline, as is shown above for the arguments `max_length` and `do_sample`.\n","\n","Below is an example of text generation using `XLNet` and its tokenizer, which includes calling `generate()` directly:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oRxmtRBlkIDX"},"outputs":[],"source":["from transformers import AutoModelForCausalLM, AutoTokenizer\n","\n","model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")\n","tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")"]},{"cell_type":"code","source":["# Ensure pad_token is defined\n","tokenizer.pad_token = tokenizer.eos_token\n","model.config.pad_token_id = tokenizer.eos_token_id\n","\n","# Prompt\n","prompt = \"Today the weather is really nice and I am planning on \"\n","\n","# Tokenize with attention mask\n","inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, add_special_tokens=False)\n","\n","# Generate\n","outputs = model.generate(\n","    inputs[\"input_ids\"],\n","    attention_mask=inputs[\"attention_mask\"],\n","    max_length=250,\n","    do_sample=True,\n","    top_p=0.9,\n","    pad_token_id=tokenizer.pad_token_id\n",")\n","\n","# Decode only the generated part\n","generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","print(generated_text)"],"metadata":{"id":"QQ9h7VNVFrbX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dsexuhenkIDY"},"source":["## Named Entity Recognition"]},{"cell_type":"markdown","metadata":{"id":"uS70HePxkIDY"},"source":["Named Entity Recognition (NER) is the task of classifying tokens according to a class, for example, identifying a token\n","as a person, an organisation or a location. An example of a named entity recognition dataset is the CoNLL-2003 dataset,\n","which is entirely based on that task. If you would like to fine-tune a model on an NER task, you may leverage the\n","[run_ner.py](https://github.com/huggingface/transformers/tree/main/examples/pytorch/token-classification/run_ner.py) script.\n","\n","Here is an example of using pipelines to do named entity recognition, specifically, trying to identify tokens as\n","belonging to one of 9 classes:\n","\n","- O, Outside of a named entity\n","- B-MIS, Beginning of a miscellaneous entity right after another miscellaneous entity\n","- I-MIS, Miscellaneous entity\n","- B-PER, Beginning of a person's name right after another person's name\n","- I-PER, Person's name\n","- B-ORG, Beginning of an organisation right after another organisation\n","- I-ORG, Organisation\n","- B-LOC, Beginning of a location right after another location\n","- I-LOC, Location\n","\n","It leverages a fine-tuned model on CoNLL-2003, fine-tuned by [@stefan-it](https://github.com/stefan-it) from [dbmdz](https://github.com/dbmdz)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ScK5SObPkIDZ"},"outputs":[],"source":["from transformers import AutoModelForTokenClassification, AutoTokenizer\n","import torch\n","\n","model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n","tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n","\n","sequence = (\n","    \"\"\"Bangalore, officially Bengaluru, is the capital of Karnataka and one of India‚Äôs largest cities,\n","    with a population exceeding 8.5 million. Historically a quiet town under the rule of Kempe Gowda in the 16th century,\n","    it grew under the Mysore Kingdom and later British colonial influence. Post-independence, it evolved into India‚Äôs technology hub,\n","    earning the nickname ‚ÄúSilicon Valley of India.‚Äù The city hosts a diverse population with a mix of Kannada, Tamil, Telugu, and Hindi speakers,\n","    reflecting its pan-Indian character. Its pleasant climate, educational institutions, and thriving startup ecosystem make it a magnet for\n","    professionals, students, and entrepreneurs across the country..\"\"\"\n",")\n","\n","inputs = tokenizer(sequence, return_tensors=\"pt\")\n","tokens = inputs.tokens()\n","\n","outputs = model(**inputs).logits\n","predictions = torch.argmax(outputs, dim=2)"]},{"cell_type":"markdown","metadata":{"id":"7DAsg8aykIDa"},"source":["This outputs a list of each token mapped to its corresponding prediction. Differently from the pipeline, here every\n","token has a prediction as we didn't remove the \"0\"th class, which means that no particular entity was found on that\n","token.\n","\n","In the above example, `predictions` is an integer that corresponds to the predicted class. We can use the\n","`model.config.id2label` property in order to recover the class name corresponding to the class number, which is\n","illustrated below:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LEdYhHUJkIDa"},"outputs":[],"source":["for token, prediction in zip(tokens, predictions[0].numpy()):\n","  if(model.config.id2label[prediction] != 'O'):\n","    print((token, model.config.id2label[prediction]))"]},{"cell_type":"markdown","metadata":{"id":"nnh7IAp2kIDh"},"source":["## Image classification"]},{"cell_type":"markdown","metadata":{"id":"QmJU8G26kIDi"},"source":["The general process for using a model and image processor for image classification is:\n","\n","1. Instantiate an image processor and a model from the checkpoint name.\n","2. Process the image to be classified with an image processor.\n","3. Pass the input through the model and take the `argmax` to retrieve the predicted class.\n","4. Convert the class id to a class name with `id2label` to return an interpretable result."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EmnDUmrfkIDi"},"outputs":[],"source":["from transformers import AutoImageProcessor, AutoModelForImageClassification\n","import torch\n","from datasets import load_dataset\n","\n","dataset = load_dataset(\"huggingface/cats-image\")\n","image = dataset[\"test\"][\"image\"][0]"]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import matplotlib.image as mpimg\n","\n","plt.imshow(image)\n","plt.axis('off')  # Hide axes\n","plt.show()"],"metadata":{"id":"OI-uk-c_KKrl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["feature_extractor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n","model = AutoModelForImageClassification.from_pretrained(\"google/vit-base-patch16-224\")\n","\n","inputs = feature_extractor(image, return_tensors=\"pt\")\n","\n","with torch.no_grad():\n","    logits = model(**inputs).logits\n","\n","predicted_label = logits.argmax(-1).item()\n","print(model.config.id2label[predicted_label])"],"metadata":{"id":"dglgovaSJuSV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise\n","\n","1. Load a translation model from HF based on source and destination language and translate a sentence.\n","\n","2. Load Summarization model and summarize the content of the file \"gpu_shortage\""],"metadata":{"id":"aZakrS2zK7V_"}},{"cell_type":"code","source":[],"metadata":{"id":"3xOt_uKPKTtb"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":0}