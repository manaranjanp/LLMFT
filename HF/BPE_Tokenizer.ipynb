{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"MOsHUjgdIrIW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1753609616514,"user_tz":-330,"elapsed":23108,"user":{"displayName":"MANARANJAN PRADHAN","userId":"03885802779803335284"}},"outputId":"900536e9-64ac-4ed1-f852-1261342a7e94"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (4.0.0)\n","Requirement already satisfied: transformers[sentencepiece] in /usr/local/lib/python3.11/dist-packages (4.54.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n","Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n","Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n","Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n","Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n","Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.34.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (25.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers[sentencepiece]) (2024.11.6)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers[sentencepiece]) (0.21.2)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers[sentencepiece]) (0.5.3)\n","Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.11/dist-packages (from transformers[sentencepiece]) (0.2.0)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from transformers[sentencepiece]) (5.29.5)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.14)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.14.1)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.7.14)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"]}],"source":["!pip install -U datasets transformers[sentencepiece]"]},{"cell_type":"markdown","metadata":{"id":"whPRbBNbIrIl"},"source":["## Loading the dataset"]},{"cell_type":"markdown","metadata":{"id":"sfqgDSGirPQE"},"source":["# Training your own tokenizer from scratch"]},{"cell_type":"markdown","metadata":{"id":"_X6ivP84rPQF"},"source":["## Getting a corpus"]},{"cell_type":"markdown","metadata":{"id":"j18t3nTurPQF"},"source":["We will need texts to train our tokenizer. We will use the [ü§ó Datasets](https://github.com/huggingface/datasets) library to download our text data, which can be easily done with the `load_dataset` function:"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"EB-0TLO9rPQG","executionInfo":{"status":"ok","timestamp":1753609618905,"user_tz":-330,"elapsed":2380,"user":{"displayName":"MANARANJAN PRADHAN","userId":"03885802779803335284"}}},"outputs":[],"source":["from datasets import load_dataset"]},{"cell_type":"markdown","metadata":{"id":"8RHzuJsZrPQG"},"source":["For this example, we will use Wikitext-2 (which contains 4.5MB of texts so training goes fast for our example) but you can use any dataset you want (and in any language, just not English)."]},{"cell_type":"code","execution_count":3,"metadata":{"id":"QcybwRyprPQG","executionInfo":{"status":"ok","timestamp":1753609623087,"user_tz":-330,"elapsed":4180,"user":{"displayName":"MANARANJAN PRADHAN","userId":"03885802779803335284"}}},"outputs":[],"source":["dataset = load_dataset(\"wikitext\", name=\"wikitext-2-raw-v1\", split=\"train\")"]},{"cell_type":"markdown","metadata":{"id":"P2u-q7qFrPQG"},"source":["We can have a look at the dataset, which as 36,718 texts:"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"3sQRjgaSrPQG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1753609623128,"user_tz":-330,"elapsed":39,"user":{"displayName":"MANARANJAN PRADHAN","userId":"03885802779803335284"}},"outputId":"b0722b93-13ee-43fc-d6d2-147679146a3e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["Dataset({\n","    features: ['text'],\n","    num_rows: 36718\n","})"]},"metadata":{},"execution_count":4}],"source":["dataset"]},{"cell_type":"markdown","metadata":{"id":"0SQLLbKVrPQG"},"source":["To access an element, we just have to provide its index:"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"ih8HoXEcrPQG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1753609623131,"user_tz":-330,"elapsed":3,"user":{"displayName":"MANARANJAN PRADHAN","userId":"03885802779803335284"}},"outputId":"51f5dd45-a218-426c-fc57-d620eeb17b58"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'text': ' = Valkyria Chronicles III = \\n'}"]},"metadata":{},"execution_count":5}],"source":["dataset[1]"]},{"cell_type":"markdown","metadata":{"id":"8CNJ7kiKrPQH"},"source":["We can also access a slice directly, in which case we get a dictionary with the key `\"text\"` and a list of texts as value:"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"giXAwd0PrPQH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1753609623142,"user_tz":-330,"elapsed":10,"user":{"displayName":"MANARANJAN PRADHAN","userId":"03885802779803335284"}},"outputId":"d1da004a-13cb-4c4f-c9c6-5c97b1d148a5"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'text': ['',\n","  ' = Valkyria Chronicles III = \\n',\n","  '',\n","  ' Senj≈ç no Valkyria 3 : Unrecorded Chronicles ( Japanese : Êà¶Â†¥„ÅÆ„É¥„Ç°„É´„Ç≠„É•„É™„Ç¢3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . Employing the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" Calamaty Raven \" . \\n',\n","  \" The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II . While it retained the standard features of the series , it also underwent multiple adjustments , such as making the game more forgiving for series newcomers . Character designer Raita Honjou and composer Hitoshi Sakimoto both returned from previous entries , along with Valkyria Chronicles II director Takeshi Ozawa . A large team of writers handled the script . The game 's opening theme was sung by May 'n . \\n\"]}"]},"metadata":{},"execution_count":6}],"source":["dataset[:5]"]},{"cell_type":"markdown","metadata":{"id":"pf7lvzmQrPQH"},"source":["The API to train our tokenizer will require an iterator of batch of texts, for instance a list of list of texts:"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"UWbOLkNlrPQH","executionInfo":{"status":"ok","timestamp":1753609623284,"user_tz":-330,"elapsed":141,"user":{"displayName":"MANARANJAN PRADHAN","userId":"03885802779803335284"}}},"outputs":[],"source":["batch_size = 1000\n","all_texts = [dataset[i : i + batch_size][\"text\"] for i in range(0, len(dataset), batch_size)]"]},{"cell_type":"markdown","metadata":{"id":"WQWg8IRirPQH"},"source":["To avoid loading everything into memory (since the Datasets library keeps the element on disk and only load them in memory when requested), we define a Python iterator. This is particularly useful if you have a huge dataset:"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"XFY38ogNrPQH","executionInfo":{"status":"ok","timestamp":1753609623293,"user_tz":-330,"elapsed":7,"user":{"displayName":"MANARANJAN PRADHAN","userId":"03885802779803335284"}}},"outputs":[],"source":["def batch_iterator():\n","    for i in range(0, len(dataset), batch_size):\n","        yield dataset[i : i + batch_size][\"text\"]"]},{"cell_type":"markdown","metadata":{"id":"Fm6rJ4IQrPQH"},"source":["Now let's see how we can use this corpus to train a new tokenizer! There are two APIs to do this: the first one uses an existing tokenizer and will train a new version of it on your corpus in one line of code, the second is to actually build your tokenizer block by block, so lets you customize every step!"]},{"cell_type":"markdown","metadata":{"id":"362u8kH4rPQH"},"source":["## Using an existing tokenizer"]},{"cell_type":"markdown","metadata":{"id":"xrZJ2AjdrPQH"},"source":["If you want to train a tokenizer with the exact same algorithms and parameters as an existing one, you can just use the `train_new_from_iterator` API. For instance, let's train a new version of the GPT-2 tokenzier on Wikitext-2 using the same tokenization algorithm.\n","\n","First we need to load the tokenizer we want to use as a model:"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"bXBP0nK0rPQH","executionInfo":{"status":"ok","timestamp":1753609640323,"user_tz":-330,"elapsed":17028,"user":{"displayName":"MANARANJAN PRADHAN","userId":"03885802779803335284"}}},"outputs":[],"source":["from transformers import AutoTokenizer\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"]},{"cell_type":"markdown","metadata":{"id":"rdEOq7dirPQH"},"source":["Make sure that the tokenizer you picked as a *fast* version (backed by the ü§ó Tokenizers library) otherwise the rest of the notebook will not run:"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"3vQDJjODrPQH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1753609640333,"user_tz":-330,"elapsed":8,"user":{"displayName":"MANARANJAN PRADHAN","userId":"03885802779803335284"}},"outputId":"0b2b9b41-a448-4634-b180-b1149a6bd09a"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":10}],"source":["tokenizer.is_fast"]},{"cell_type":"markdown","metadata":{"id":"X6kWq2hhrPQH"},"source":["Then we feed the training corpus (either the list of list or the iterator we defined earlier) to the `train_new_from_iterator` method. We also have to specify the vocabulary size we want to use:"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"Y3wQd16crPQI","executionInfo":{"status":"ok","timestamp":1753609651376,"user_tz":-330,"elapsed":11042,"user":{"displayName":"MANARANJAN PRADHAN","userId":"03885802779803335284"}}},"outputs":[],"source":["new_tokenizer = tokenizer.train_new_from_iterator(batch_iterator(), vocab_size=25000)"]},{"cell_type":"markdown","metadata":{"id":"m0Obs15WrPQI"},"source":["And that's all there is to it! The training goes very fast thanks to the ü§ó Tokenizers library, backed by Rust.\n","\n","You now have a new tokenizer ready to preprocess your data and train a language model. You can feed it input texts as usual:"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"t3tFGCXYrPQI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1753609651390,"user_tz":-330,"elapsed":13,"user":{"displayName":"MANARANJAN PRADHAN","userId":"03885802779803335284"}},"outputId":"b163164a-925d-4e11-af78-c5feaa3d1b7c"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'input_ids': [[], [301, 8639, 9504, 3050, 301, 315], [], [4720, 74, 4825, 889, 8639, 491, 529, 672, 6944, 475, 267, 9504, 374, 2809, 529, 10879, 231, 100, 162, 255, 113, 14391, 4046, 113, 4509, 95, 18351, 4509, 256, 4046, 99, 4046, 22234, 96, 19, 264, 6437, 272, 8639, 281, 261, 3518, 2035, 491, 373, 264, 5162, 3305, 290, 344, 8639, 9504, 3050, 2616, 1822, 264, 364, 259, 14059, 1559, 340, 2393, 1527, 737, 1961, 370, 805, 3604, 288, 7577, 14, 54, 782, 337, 261, 4840, 15585, 272, 19958, 284, 1404, 1696, 284, 1822, 264, 385, 364, 261, 1431, 737, 284, 261, 8639, 906, 272, 2531, 1858, 286, 261, 1112, 9658, 281, 14059, 288, 1626, 340, 645, 6556, 344, 520, 14434, 264, 261, 1485, 3436, 7515, 290, 261, 518, 737, 288, 4750, 261, 302, 22039, 302, 264, 259, 21720, 1743, 3836, 5654, 261, 4259, 281, 4742, 490, 724, 261, 3581, 1351, 283, 1114, 579, 952, 4010, 1985, 2563, 288, 453, 2128, 807, 935, 261, 7655, 3836, 302, 2038, 314, 271, 89, 22414, 302, 272, 315], [324, 737, 1022, 1984, 284, 1525, 264, 7663, 610, 259, 1241, 4816, 281, 261, 693, 3654, 326, 8639, 9504, 1243, 272, 1894, 385, 7631, 261, 3684, 2303, 281, 261, 906, 264, 385, 534, 9638, 5354, 16654, 1030, 264, 844, 344, 1878, 261, 737, 667, 10407, 1315, 337, 906, 727, 3210, 383, 272, 13353, 8814, 8187, 2591, 6086, 74, 298, 288, 7508, 10103, 17447, 304, 11550, 9013, 920, 1898, 403, 1445, 22645, 264, 1071, 359, 8639, 9504, 1243, 2499, 21197, 5400, 19526, 5224, 272, 303, 1241, 990, 281, 3839, 8713, 261, 3418, 272, 324, 737, 331, 83, 2574, 3535, 321, 8351, 370, 1073, 331, 78, 272, 315]], 'attention_mask': [[], [1, 1, 1, 1, 1, 1], [], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"]},"metadata":{},"execution_count":12}],"source":["new_tokenizer(dataset[:5][\"text\"])"]},{"cell_type":"markdown","metadata":{"id":"BGXOJeigrPQU"},"source":["## Building your tokenizer from scratch"]},{"cell_type":"markdown","metadata":{"id":"FssB7M-TrPQU"},"source":["To understand how to build your tokenizer from scratch, we have to dive a little bit more in the ü§ó Tokenizers library and the tokenization pipeline. This pipeline takes several steps:\n","\n","- **Normalization**: Executes all the initial transformations over the initial input string. For example when you need to lowercase some text, maybe strip it, or even apply one of the common unicode normalization process, you will add a Normalizer.\n","- **Pre-tokenization**: In charge of splitting the initial input string. That's the component that decides where and how to pre-segment the origin string. The simplest example would be to simply split on spaces.\n","- **Model**: Handles all the sub-token discovery and generation, this is the part that is trainable and really dependent of your input data.\n","- **Post-Processing**: Provides advanced construction features to be compatible with some of the Transformers-based SoTA models. For instance, for BERT it would wrap the tokenized sentence around [CLS] and [SEP] tokens.\n","\n","And to go in the other direction:\n","\n","- **Decoding**: In charge of mapping back a tokenized input to the original string. The decoder is usually chosen according to the `PreTokenizer` we used previously.\n","\n","For the training of the model, the ü§ó Tokenizers library provides a `Trainer` class that we will use.\n","\n","All of these building blocks can be combined to create working tokenization pipelines. To give you some examples, we will show three full pipelines here: how to replicate GPT-2, BERT and T5 (which will give you an example of BPE, WordPiece and Unigram tokenizer)."]},{"cell_type":"markdown","metadata":{"id":"sGOVVm87rPQV"},"source":["Let's have a look at how we can create a WordPiece tokenizer like the one used for training BERT. The first step is to create a `Tokenizer` with an empty `WordPiece` model:"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"4WSGOSm9rPQV","executionInfo":{"status":"ok","timestamp":1753609651406,"user_tz":-330,"elapsed":14,"user":{"displayName":"MANARANJAN PRADHAN","userId":"03885802779803335284"}}},"outputs":[],"source":["from tokenizers import decoders, models, normalizers, pre_tokenizers, processors, trainers, Tokenizer"]},{"cell_type":"markdown","metadata":{"id":"9iTX61nDrPQW"},"source":["### BPE model like GPT-2"]},{"cell_type":"markdown","metadata":{"id":"wUhV-hVfrPQW"},"source":["Let's now have a look at how we can create a BPE tokenizer like the one used for training GPT-2. The first step is to create a `Tokenizer` with an empty `BPE` model:"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"oP4LXErJrPQW","executionInfo":{"status":"ok","timestamp":1753609651421,"user_tz":-330,"elapsed":10,"user":{"displayName":"MANARANJAN PRADHAN","userId":"03885802779803335284"}}},"outputs":[],"source":["tokenizer = Tokenizer(models.BPE())"]},{"cell_type":"markdown","metadata":{"id":"k-ENdfxVrPQW"},"source":["Like before, we have to add the optional normalization (not used in the case of GPT-2) and we need to specify a pre-tokenizer before training. In the case of GPT-2, the pre-tokenizer used is a byte level pre-tokenizer:"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"dyU_A2zorPQW","executionInfo":{"status":"ok","timestamp":1753609651428,"user_tz":-330,"elapsed":6,"user":{"displayName":"MANARANJAN PRADHAN","userId":"03885802779803335284"}}},"outputs":[],"source":["tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)"]},{"cell_type":"markdown","metadata":{"id":"SVilk70krPQW"},"source":["If we want to have a quick look at how it preprocesses the inputs, we can call the `pre_tokenize_str` method:"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"BmKPEDHjrPQX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1753609651452,"user_tz":-330,"elapsed":15,"user":{"displayName":"MANARANJAN PRADHAN","userId":"03885802779803335284"}},"outputId":"9ea05058-13ac-48e2-e08a-31cb51c0721a"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('This', (0, 4)),\n"," ('ƒ†is', (4, 7)),\n"," ('ƒ†an', (7, 10)),\n"," ('ƒ†example', (10, 18)),\n"," ('!', (18, 19))]"]},"metadata":{},"execution_count":16}],"source":["tokenizer.pre_tokenizer.pre_tokenize_str(\"This is an example!\")"]},{"cell_type":"markdown","metadata":{"id":"w5r_j2E6rPQX"},"source":["We used the same default as for GPT-2 for the prefix space, so you can see that each word gets an initial `'ƒ†'` added at the beginning, except the first one.\n","\n","We can now train our tokenizer! This time we use a `BpeTrainer`."]},{"cell_type":"code","execution_count":17,"metadata":{"id":"V0kOcZx3rPQX","executionInfo":{"status":"ok","timestamp":1753609662527,"user_tz":-330,"elapsed":11073,"user":{"displayName":"MANARANJAN PRADHAN","userId":"03885802779803335284"}}},"outputs":[],"source":["trainer = trainers.BpeTrainer(vocab_size=25000, special_tokens=[\"[EOS]\"])\n","tokenizer.train_from_iterator(batch_iterator(), trainer=trainer)"]},{"cell_type":"markdown","metadata":{"id":"Os2Z5NF6rPQX"},"source":["To finish the whole pipeline, we have to include the post-processor and decoder:"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"_Wp8jEe5rPQX","executionInfo":{"status":"ok","timestamp":1753609662539,"user_tz":-330,"elapsed":1,"user":{"displayName":"MANARANJAN PRADHAN","userId":"03885802779803335284"}}},"outputs":[],"source":["tokenizer.post_processor = processors.ByteLevel(trim_offsets=False)\n","tokenizer.decoder = decoders.ByteLevel()"]},{"cell_type":"markdown","metadata":{"id":"laYFtvuyrPQX"},"source":["And like before, we finish by wrapping this in a Transformers tokenizer object:"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"o4F582kUrPQX","executionInfo":{"status":"ok","timestamp":1753609662887,"user_tz":-330,"elapsed":16,"user":{"displayName":"MANARANJAN PRADHAN","userId":"03885802779803335284"}}},"outputs":[],"source":["from transformers import GPT2TokenizerFast\n","\n","bpe_tokenizer = GPT2TokenizerFast(tokenizer_object=tokenizer)"]},{"cell_type":"markdown","metadata":{"id":"fQv5n9q4rPQY"},"source":["## Use your new tokenizer to train a language model!"]},{"cell_type":"markdown","metadata":{"id":"dU_rRYMQrPQY"},"source":["You can either use your new tokenizer in the language modeling from scratch notebook [Link to come] or use the `--tokenizer_name` argument in the [language modeling scripts](https://github.com/huggingface/transformers/tree/master/examples/pytorch/language-modeling) to use it there to train a model from scratch."]},{"cell_type":"code","execution_count":20,"metadata":{"id":"SSzOA7_rrPQY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1753609662972,"user_tz":-330,"elapsed":80,"user":{"displayName":"MANARANJAN PRADHAN","userId":"03885802779803335284"}},"outputId":"3841d0f7-bd0d-4329-f4f8-ada933a73faa"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'input_ids': [[], [238, 8576, 9441, 2987, 238, 252], [], [4657, 74, 4762, 826, 8576, 428, 466, 609, 6881, 412, 204, 9441, 311, 2746, 466, 10816, 168, 99, 150, 192, 112, 14328, 3983, 112, 4446, 94, 18288, 4446, 193, 3983, 98, 3983, 22171, 95, 19, 201, 6374, 209, 8576, 218, 198, 3455, 1972, 428, 310, 201, 5099, 3242, 227, 281, 8576, 9441, 2987, 2553, 1759, 201, 301, 196, 13996, 1496, 277, 2330, 1464, 674, 1898, 307, 742, 3541, 225, 7514, 14, 54, 719, 274, 198, 4777, 15522, 209, 19895, 221, 1341, 1633, 221, 1759, 201, 322, 301, 198, 1368, 674, 221, 198, 8576, 843, 209, 2468, 1795, 223, 198, 1049, 9595, 218, 13996, 225, 1563, 277, 582, 6493, 281, 457, 14371, 201, 198, 1422, 3373, 7452, 227, 198, 455, 674, 225, 4687, 198, 239, 21976, 239, 201, 196, 21657, 1680, 3773, 5591, 198, 4196, 218, 4679, 427, 661, 198, 3518, 1288, 220, 1051, 516, 889, 3947, 1922, 2500, 225, 390, 2065, 744, 872, 198, 7592, 3773, 239, 1975, 251, 208, 89, 22351, 239, 209, 252], [261, 674, 959, 1921, 221, 1462, 201, 7600, 547, 196, 1178, 4753, 218, 198, 630, 3591, 263, 8576, 9441, 1180, 209, 1831, 322, 7568, 198, 3621, 2240, 218, 198, 843, 201, 322, 471, 9575, 5291, 16591, 967, 201, 781, 281, 1815, 198, 674, 604, 10344, 1252, 274, 843, 664, 3147, 320, 209, 13290, 8751, 8124, 2528, 6023, 74, 235, 225, 7445, 10040, 17384, 241, 11487, 8950, 857, 1835, 340, 1382, 22582, 201, 1008, 296, 8576, 9441, 1180, 2436, 21134, 5337, 19463, 5161, 209, 240, 1178, 927, 218, 3776, 8650, 198, 3355, 209, 261, 674, 268, 83, 2511, 3472, 258, 8288, 307, 1010, 268, 78, 209, 252]], 'attention_mask': [[], [1, 1, 1, 1, 1, 1], [], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"]},"metadata":{},"execution_count":20}],"source":["bpe_tokenizer(dataset[:5][\"text\"])"]},{"cell_type":"code","source":["bpe_tokenizer.SPECIAL_TOKENS_ATTRIBUTES"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wntKJTNjOiOy","executionInfo":{"status":"ok","timestamp":1753609662972,"user_tz":-330,"elapsed":7,"user":{"displayName":"MANARANJAN PRADHAN","userId":"03885802779803335284"}},"outputId":"01af4e71-ade3-4aba-e7b9-ef29def944f6"},"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['bos_token',\n"," 'eos_token',\n"," 'unk_token',\n"," 'sep_token',\n"," 'pad_token',\n"," 'cls_token',\n"," 'mask_token',\n"," 'additional_special_tokens']"]},"metadata":{},"execution_count":21}]},{"cell_type":"code","source":["bpe_tokenizer.vocab_size"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lHd8sR26O77H","executionInfo":{"status":"ok","timestamp":1753609662972,"user_tz":-330,"elapsed":3,"user":{"displayName":"MANARANJAN PRADHAN","userId":"03885802779803335284"}},"outputId":"ca11f89a-3bf7-4c26-971b-5ad35d0f2222"},"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["25000"]},"metadata":{},"execution_count":22}]},{"cell_type":"code","source":["from itertools import islice\n","\n","for key, value in islice(bpe_tokenizer.vocab.items(), 25):\n","    print(f\"{key}: {value}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iP0nsX0PO5xg","executionInfo":{"status":"ok","timestamp":1753609697173,"user_tz":-330,"elapsed":18,"user":{"displayName":"MANARANJAN PRADHAN","userId":"03885802779803335284"}},"outputId":"8162146d-acc9-4f3f-ef2c-699997587bf1"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["ƒ†Yard: 22508\n","ƒ†Specific: 21728\n","ƒ†Flotilla: 19671\n","ƒ†Chutzpah: 20345\n","rance: 4092\n","igade: 1887\n","ƒ†exhibits: 16588\n","ƒ†Beth: 11660\n","ƒ†accur: 6781\n","ƒ†Draws: 24539\n","ƒ†weaker: 21409\n","umm: 16377\n","itage: 6576\n","ƒ†speeds: 18895\n","ƒ†Bing: 24899\n","ƒ†destruct: 15077\n","tha: 17367\n","incl: 12430\n","ƒ†intake: 22459\n","ƒ†Khan: 9964\n","ƒ†Bes: 11241\n","ƒ†occasion: 4584\n","ƒ†amateur: 8139\n","ƒ†containers: 19819\n","ƒ†Walk: 6586\n"]}]},{"cell_type":"code","source":["vocab = bpe_tokenizer.get_vocab()\n","sorted_vocab = sorted(vocab.items(), key=lambda item: item[1])\n","\n","# Write cleaned vocab to a file\n","with open(\"vocab.txt\", \"w\", encoding=\"utf-8\") as f:\n","    for token, index in sorted_vocab:\n","        # Replace ƒ† with space for readability\n","        cleaned_token = token.replace(\"ƒ†\", \"_\")\n","        f.write(f\"{cleaned_token}\\n\")"],"metadata":{"id":"Fq_sPTmaPAzD","executionInfo":{"status":"ok","timestamp":1753609746102,"user_tz":-330,"elapsed":75,"user":{"displayName":"MANARANJAN PRADHAN","userId":"03885802779803335284"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"5Oyt6e_EQQ5I","executionInfo":{"status":"ok","timestamp":1753609663189,"user_tz":-330,"elapsed":8,"user":{"displayName":"MANARANJAN PRADHAN","userId":"03885802779803335284"}}},"execution_count":24,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"}},"nbformat":4,"nbformat_minor":0}