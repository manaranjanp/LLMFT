{"cells":[{"cell_type":"markdown","metadata":{"id":"Xws2p6SgbNF-"},"source":["# IMDB Classification - Bag of Words and Embeddings\n","\n","This tutorial will go through steps for building a deep learning model for sentiment Analysis. We will classify IMDB movie reviews as either positive or negative. This tutorial will be used for teaching during the workshop.\n","\n","The tutorial has taken contents from various places including the tutorial from http://www.hvass-labs.org/ for the purpose of teaching in the deep learning class.\n","\n","The topics addressed in the tutorial:\n","\n","1. Basic exploration of the IMDB movies dataset.\n","2. Tokenization, text to sequences, padding and truncating\n","3. Building NN Model using Bag Of Words\n","4. Building NN Model using Embeddings\n","5. Peeping to Word Embeddings\n","\n","We will be exploring mostly how to use Bag of Words and Word Embeddings vector representation of texts and build plain vanila NN models. In the future tutorials, we will explore RNN, LSTM models in the future."]},{"cell_type":"markdown","metadata":{"id":"ZWhTaM5CbNGF"},"source":["### IMDB Movie Reviews\n","\n","The dataset is available at https://www.kaggle.com/c/word2vec-nlp-tutorial/data\n","\n","The labeled data set consists of 50,000 IMDB movie reviews, specially selected for sentiment analysis. The sentiment of reviews is binary, meaning the IMDB rating < 5 results in a sentiment score of 0, and rating >=7 have a sentiment score of 1. No individual movie has more than 30 reviews.\n","\n","**Data Fields**\n","\n","- id - Unique ID of each review\n","- sentiment - Sentiment of the review; 1 for positive reviews and 0 for negative reviews\n","- review - Text of the review"]},{"cell_type":"code","source":["%load_ext tensorboard"],"metadata":{"id":"8_-rpXepT-pD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hb91-UQ8bNGG"},"source":["### Loading the dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cp3EZNi4bNGH"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sn"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RLE_SJJobNGJ"},"outputs":[],"source":["imdb_df = pd.read_csv('labeledTrainData.tsv',\n","                      sep = '\\t')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VGRs67fWbNGJ"},"outputs":[],"source":["pd.set_option('display.max_colwidth', 500)\n","imdb_df.head(5)"]},{"cell_type":"markdown","metadata":{"id":"uFXFNPyabNGL"},"source":["### Data Tokenization"]},{"cell_type":"markdown","metadata":{"id":"sChuq5RdbNGM"},"source":["The text data need to be converted into vectors using either bag of words or embeddings model. We will first explore bag of words (BOW) model. In the BOW model, a sentence will be represented as a vector with the words (also called tokens) as dimensions of the vectors.\n","\n","For the purpose of creating vectors, we need to tokenize the sentences first and find out all unique tokens (words) used across all sentences. The corpus of unquie words used could very large, so we can limit the corpus of tokens by using only the most popular (frequently used) words. In this example, we will use 10000 words."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zFabd9jBbNGN"},"outputs":[],"source":["import os\n","#os.environ[\"KERAS_BACKEND\"] = \"tensorflow\""]},{"cell_type":"code","source":["import keras\n","print(keras.__version__)"],"metadata":{"id":"NYUtiuH6MZ-K"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NFPJvJXGbNGd"},"source":["### Encode Y Variable"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FkUnnz-3bNGd"},"outputs":[],"source":["y = np.array(imdb_df.sentiment)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mmJrmTSwbNGd"},"outputs":[],"source":["y[0:5]"]},{"cell_type":"markdown","metadata":{"id":"4bAhxbiLbNGe"},"source":["How many classes available?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O_zmFiWZbNGe"},"outputs":[],"source":["imdb_df.sentiment.unique()"]},{"cell_type":"markdown","metadata":{"id":"DqIzmu4_bNGi"},"source":["Now we will pad or truncate. But padding or truncating can be done at the beginning of the sentence or at the end of the sentences. *pre* or *post* can be used to specify the padding and truncating the beginning or end of sentence."]},{"cell_type":"code","source":["max_num_tokens = 10000\n","max_review_length = 500"],"metadata":{"id":"-9sPOXEwQWGQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from keras.layers import TextVectorization"],"metadata":{"id":"Wht9SNbQsNX_"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z24TwEJ_bNGj"},"outputs":[],"source":["vectorize_layer = TextVectorization(max_tokens = max_num_tokens,\n","                                    output_mode='int',\n","                                    output_sequence_length = max_review_length,\n","                                    standardize='lower_and_strip_punctuation',\n","                                    split='whitespace')"]},{"cell_type":"code","source":["vectorize_layer.adapt(list(imdb_df.review))"],"metadata":{"id":"VD4KUlEuszDS"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I346GV1YbNGj"},"outputs":[],"source":["vectorize_layer.get_vocabulary()[0:20]"]},{"cell_type":"code","source":["vectorize_layer([\"I like the movie gladiator\"])[0][0:50]"],"metadata":{"id":"NkHZB_VnQ642"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vectorize_layer(imdb_df.review[0:1])[0][0:50]"],"metadata":{"id":"YV_pFlVBRAnQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NPYtrILXbNGk"},"source":["### Split Datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XFM0pgKpbNGk"},"outputs":[],"source":["from sklearn.model_selection import train_test_split"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_4NNXJpbbNGk"},"outputs":[],"source":["X_train, X_test, y_train, y_test = train_test_split(imdb_df.review,\n","                                                    imdb_df.sentiment,\n","                                                    test_size = 0.2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xe-yzMaPbNGl"},"outputs":[],"source":["X_train.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0GLjMTx3bNGl"},"outputs":[],"source":["X_test.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_ddvUY_jbNGl"},"outputs":[],"source":["input_shape = X_train.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dnV61_CxbNGl"},"outputs":[],"source":["input_shape"]},{"cell_type":"markdown","metadata":{"id":"vwPwPOT3bNGo"},"source":["### Using Embeddings\n","\n","In Word embeddings, words are represented by a vector i.e. series of numbers (weights). The vectors represent words in a N dimension space, in which similar meaning words are places nearer to each other while the dissimilar words are kept far. The dimensions in the space represent some latent factors, by which the words could be defined. All words are assigned some weights in each each latent factors. Words that share some common meaning have similar weights across common factors.\n","\n","The word embeddings weights can be estimated during the NN model building. There are also pre-built word embeddings are available, which can be used in the model. We will discuss about the pre-built word embeddings later in the tutorial.\n","\n","Word embeddings are commonly used in many Natural Language Processing (NLP) tasks because they are found to be useful representations of words and often lead to better performance in the various tasks performed. Given its widespread use, this post seeks to introduce the concept of word embeddings to the prospective NLP practitioner.\n","\n","Here are couple of good references to understand embeddings\n","\n","https://medium.com/huggingface/universal-word-sentence-embeddings-ce48ddc8fc3a\n","\n","(Bag of words) -> Embeddings (8) -> Dense Layer(16) ->  Relu -> Dense Layer(1) -> Sigmoid"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IEqDhGohbNGo"},"outputs":[],"source":["from keras.layers import Embedding\n","from keras.optimizers import SGD\n","from keras.models import Sequential\n","from keras.layers import Flatten, Dense, Activation, Dropout"]},{"cell_type":"code","source":["vectorize_layer = TextVectorization(max_tokens = max_num_tokens,\n","                                    output_mode='int',\n","                                    output_sequence_length = max_review_length,\n","                                    standardize='lower_and_strip_punctuation',\n","                                    split='whitespace')"],"metadata":{"id":"enHfN30_ywWi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vectorize_layer.adapt(list(X_train))"],"metadata":{"id":"62cJxtPah2S7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_ds = vectorize_layer(X_train)"],"metadata":{"id":"elIhR1kXSdwV"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"22K2lF8XbNGo"},"outputs":[],"source":["keras.backend.clear_session()  # clear default graph\n","\n","emb_model = Sequential()\n","emb_model.add(keras.Input(shape=(max_review_length,)))\n","# We specify the maximum input length to our Embedding layer\n","# so we can later flatten the embedded inputs\n","emb_model.add(Embedding(max_num_tokens, 8))\n","\n","# After the Embedding layer,\n","# our activations have shape `(samples, maxlen, 8)`.\n","\n","# We flatten the 3D tensor of embeddings\n","# into a 2D tensor of shape `(samples, maxlen * 8)`\n","emb_model.add(Flatten())\n","\n","emb_model.add(Dense(16))\n","emb_model.add(Activation('relu'))\n","\n","# We add the classifier on top\n","emb_model.add(Dense(1))\n","emb_model.add(Activation('sigmoid'))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n3g03KhkbNGo"},"outputs":[],"source":["emb_model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"41ySXfC8bNGp"},"outputs":[],"source":["sgd = SGD(learning_rate=0.01, momentum=0.8)"]},{"cell_type":"code","source":["emb_model.compile(optimizer=sgd,\n","              loss='binary_crossentropy',\n","              metrics=['accuracy'])"],"metadata":{"id":"eDM7boSKg2oE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["callbacks_list = [keras.callbacks.ReduceLROnPlateau(monitor='val_loss',\n","                                    factor=0.1,\n","                                    patience=2),\n","                  keras.callbacks.EarlyStopping(monitor='val_loss',\n","                                patience=6),\n","                  keras.callbacks.TensorBoard(log_dir=\"klogs\", histogram_freq=1)]"],"metadata":{"id":"w3EnFh2CSLLE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["emb_history = emb_model.fit(train_ds,\n","                            y_train,\n","                            epochs=20,\n","                            batch_size=32,\n","                            callbacks = callbacks_list,\n","                            validation_split=0.3)"],"metadata":{"id":"vwicElFTg4A8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%tensorboard --logdir .klogs"],"metadata":{"id":"RMeMumRcTuXA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iWIZ2q1obNGr"},"source":["#### Conclusion:\n","\n","The model is overfitting. The training accuracy is about 98%, whereas the validation accuracy is 80%."]},{"cell_type":"markdown","metadata":{"id":"W5gqkyfRbNGt"},"source":["### Model 4"]},{"cell_type":"markdown","metadata":{"id":"1QG4w5AQbNGu"},"source":["Add a dropout layer as a regularization layer for dealing with overfitting."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z6q8qQkFbNGu"},"outputs":[],"source":["keras.backend.clear_session()  # clear default graph\n","\n","emb_model_2 = Sequential()\n","emb_model_2.add(keras.Input(shape=(max_review_length,)))\n","# We specify the maximum input length to our Embedding layer\n","# so we can later flatten the embedded inputs\n","emb_model_2.add(Embedding(max_num_tokens, 8))\n","\n","# After the Embedding layer,\n","# our activations have shape `(samples, maxlen, 8)`.\n","\n","# We flatten the 3D tensor of embeddings\n","# into a 2D tensor of shape `(samples, maxlen * 8)`\n","emb_model_2.add(Flatten())\n","\n","emb_model_2.add(Dense(16))\n","emb_model_2.add(Activation('relu'))\n","\n","emb_model_2.add(Dropout(0.8))\n","\n","# We add the classifier on top\n","emb_model_2.add(Dense(1))\n","emb_model_2.add(Activation('sigmoid'))"]},{"cell_type":"code","source":["emb_model_2.compile(optimizer=\"adam\",\n","              loss='binary_crossentropy',\n","              metrics=['accuracy'])"],"metadata":{"id":"UhmJI6bJU_5A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["callbacks_list = [keras.callbacks.ReduceLROnPlateau(monitor='val_loss',\n","                                    factor=0.1,\n","                                    patience=2),\n","                  keras.callbacks.EarlyStopping(monitor='val_loss',\n","                                patience=6),\n","                  keras.callbacks.TensorBoard(log_dir=\"klogs1\", histogram_freq=1)]"],"metadata":{"id":"KZXNdRxGVYo9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["emb_history = emb_model_2.fit(train_ds,\n","                              y_train,\n","                              epochs=20,\n","                              batch_size=32,\n","                              callbacks = callbacks_list,\n","                              validation_split=0.3)"],"metadata":{"id":"ox6_boCkVVDO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%tensorboard --logdir klogs1"],"metadata":{"id":"Cxa_8mgUXaNq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VfuTCs6bbNGw"},"source":["### Checking performance on test set"]},{"cell_type":"markdown","metadata":{"id":"S1MK2xuPbNGx"},"source":["We will use the model 4 for checking performance on test set and making predictions."]},{"cell_type":"code","source":["test_ds =  vectorize_layer(X_test)"],"metadata":{"id":"jK1P_waeVlaa"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"638k8hfibNGy"},"outputs":[],"source":["result = emb_model_2.evaluate(test_ds, y_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rlkSskfvbNGy"},"outputs":[],"source":["print(\"Accuracy: {0:.2%}\".format(result[1]))"]},{"cell_type":"markdown","metadata":{"id":"NZJwgf6LbNGy"},"source":["### Predicting Test Data and Confusion Matrix\n","\n","We will predict the classes using model 4 and build the confusion matrix to understand precision and recall."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5D4gtMO7bNGy"},"outputs":[],"source":["y_pred_probs = emb_model_2.predict(test_ds)"]},{"cell_type":"code","source":["y_pred = np.where(y_pred_probs >= 0.5, 1,0)"],"metadata":{"id":"vz47l-JLzLjq"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5o5C2XS-bNGz"},"outputs":[],"source":["from sklearn import metrics\n","\n","cm = metrics.confusion_matrix( y_test,\n","                            y_pred, labels = [1,0] )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uWUsDvmIbNGz"},"outputs":[],"source":["sn.heatmap(cm, annot=True,\n","           fmt='.2f',\n","           xticklabels = [\"Positive\", \"Negative\"] ,\n","           yticklabels = [\"Positive\", \"Negative\"] )\n","\n","plt.ylabel('True label')\n","plt.xlabel('Predicted label');\n","plt.title( 'Confusion Matrix for Sentiment Classification');"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bH30-JUqbNG0"},"outputs":[],"source":["from sklearn.metrics import classification_report"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_X8f5DIAbNG0"},"outputs":[],"source":["print( classification_report(y_test,y_pred))"]},{"cell_type":"markdown","metadata":{"id":"h8nIMReBbNG0"},"source":["# Peeping into Embeddings"]},{"cell_type":"markdown","metadata":{"id":"k2IF1oUSbNG0"},"source":["We will look at the embeddings estimated for different words and if they are placed neared or far as per their meaning."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5eZrtVIDbNG1"},"outputs":[],"source":["layer_embedding = emb_model_2.get_layer('embedding')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VAAQDiefbNG1"},"outputs":[],"source":["weights_embedding = layer_embedding.get_weights()[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zY1o7YHibNG1"},"outputs":[],"source":["weights_embedding.shape"]},{"cell_type":"code","source":["vocab = vectorize_layer.get_vocabulary()\n","vocab[0:20]"],"metadata":{"id":"23cAF5NcWu2y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vocab.index(\"the\")"],"metadata":{"id":"_nHD7gjoXDFm"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GKT4N7UgbNG1"},"outputs":[],"source":["def get_embeddings( word ):\n","    token = vocab.index(word)\n","    return weights_embedding[token]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bp_dXCnLbNG2"},"outputs":[],"source":["good = get_embeddings('good')\n","good"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OX8A8E_sbNG2"},"outputs":[],"source":["great = get_embeddings('great')\n","great"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HBXOz72mbNG2"},"outputs":[],"source":["bad = get_embeddings('bad')\n","bad"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1M1lVQ9QbNG2"},"outputs":[],"source":["terrible = get_embeddings('terrible')\n","terrible"]},{"cell_type":"markdown","metadata":{"id":"fJTsCWbRbNG3"},"source":["We will calculate the euclidean distance between the word embeddings."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PlUAIsaQbNG3"},"outputs":[],"source":["from scipy.spatial.distance import cdist"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ta0bELBUbNG3"},"outputs":[],"source":["def get_distance( word1, word2 ):\n","\n","    word1_token = vocab.index(word1)\n","    word2_token = vocab.index(word2)\n","\n","    return cdist([weights_embedding[word1_token]],\n","                 [weights_embedding[word2_token]],\n","                 metric = 'euclidean')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"scxQLoi5bNG3"},"outputs":[],"source":["get_distance( 'good',\n","             'awesome' )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dstcHCGIbNG4"},"outputs":[],"source":["get_distance( 'good', 'bad' )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_HBK7zi6bNG4"},"outputs":[],"source":["get_distance( 'bad', 'terrible' )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mxsWImAnbNG5"},"outputs":[],"source":["get_distance( 'great', 'terrible' )"]},{"cell_type":"markdown","metadata":{"id":"jzBygz6vbNG5"},"source":["It can be observed that the words *good* and *great* are places together, while *bad* and *terrible* are place together. And the words *good* and *terrible* are place far. This indicates the embeddings have incorporated the meaning of the words as per how they are used in the sentences expressing positive and negative sentiments."]},{"cell_type":"markdown","source":["## Storing Embeddings"],"metadata":{"id":"5Wnp9kpRYmvc"}},{"cell_type":"code","source":["import numpy as np\n","\n","# Assume you used a TextVectorization layer\n","vocab = vectorize_layer.get_vocabulary()  # list of words\n","\n","print(len(vocab))\n","\n","# Save embeddings\n","weights_embedding = layer_embedding.get_weights()[0]\n","\n","print(weights_embedding.shape)\n","\n","np.savetxt(\"tensor.tsv\", weights_embedding, delimiter=\"\\t\")\n","\n","# Save metadata\n","with open(\"metadata.tsv\", \"w\", encoding='utf-8') as f:\n","    for word in vocab:\n","      if word == \"\":\n","        f.write(f\"[sp]\\n\")\n","      else:\n","        f.write(f\"{word}\\n\")"],"metadata":{"id":"FwgSBJSVYrqS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6sjQ8GMqbNG5"},"source":["Some more examples expressing sentiments."]},{"cell_type":"markdown","source":["### Participant Exercise: 1\n","\n","- Build a model with an embedding layer of 16 or 32\n","- Add one more dense layer\n","- Change the number of neurons in dense layer\n","- Build a model and check accuracy\n","\n","\n","### Participant Exercise: 2\n","\n","- Explore words, their embeddings and distances between them."],"metadata":{"id":"B58kMigq3qnr"}},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"ZUywYW9tbNHA"},"source":["## Excellent References\n","\n","For further exploration and better understanding, you can use the following references.\n","\n","- Glossary of Deep Learning: Word Embedding\n","\n","    https://medium.com/deeper-learning/glossary-of-deep-learning-word-embedding-f90c3cec34ca\n","\n","\n","- wevi: word embedding visual inspector\n","\n","    https://ronxin.github.io/wevi/  \n","    \n","    \n","- Learning Word Embedding    \n","\n","    https://lilianweng.github.io/lil-log/2017/10/15/learning-word-embedding.html\n","\n","\n","- On the contribution of neural networks and word embeddings in Natural Language Processing\n","\n","    https://medium.com/@josecamachocollados/on-the-contribution-of-neural-networks-and-word-embeddings-in-natural-language-processing-c8bb1b85c61c"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"},"colab":{"provenance":[]},"accelerator":"GPU","gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}