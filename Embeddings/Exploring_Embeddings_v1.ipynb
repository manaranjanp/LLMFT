{"cells":[{"cell_type":"markdown","metadata":{"id":"gvSXQthsbNG6"},"source":["## Pre trained embeddings"]},{"cell_type":"markdown","metadata":{"id":"1Qj6SugubNG6"},"source":["Word embeddings are generally computed using word-occurrence statistics (observations about what words co-occur in sentences or documents), using a variety of  techniques, some involving neural networks, others not. The idea of a dense, lowdimensional embedding space for words, computed in an unsupervised way, was initially explored by Bengio et al. in the early 2000s,1 but it only started to take off in research and industry applications after the release of one of the most famous and successful word-embedding schemes: the Word2vec algorithm (https://code.google.com/ archive/p/word2vec), developed by Tomas Mikolov at Google in 2013. Word2vec dimensions capture specific semantic properties, such as gender.\n","\n","There are various precomputed databases of word embeddings that you can download and use in a Keras Embedding layer. Word2vec is one of them. Another popular one is called Global Vectors for Word Representation (GloVe, https://nlp.stanford.edu/projects/glove), which was developed by Stanford researchers in 2014. This embedding technique is based on factorizing a matrix of word co-occurrence statistics. Its developers have made available precomputed embeddings for millions of English tokens, obtained from Wikipedia data and Common Crawl data.\n","\n","One of the most widely used pretrained word embeddings is Glove and can be downloaded from https://nlp.stanford.edu/projects/glove/\n","\n","GloVe is pre-computed embeddings from 2014 English Wikipedia. It's a 822MB zip file named glove.6B.zip, containing 100-dimensional embedding vectors for 400,000 words (or non-word tokens)."]},{"cell_type":"code","source":["!wget https://nlp.stanford.edu/data/glove.6B.zip"],"metadata":{"id":"FA3EGyDf7wHd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!mkdir glove\n","!unzip glove.6B.zip -d glove/"],"metadata":{"id":"uJN8PVIW75jw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!head -20 /content/glove/glove.6B.50d.txt"],"metadata":{"id":"nJuA1AaVAyl3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Exploring the embeddings"],"metadata":{"id":"YqeTLMGTHKs9"}},{"cell_type":"code","source":["!pip install -U pandas"],"metadata":{"id":"ytKP5TD9MegE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install numpy==1.26.4"],"metadata":{"id":"6jjguiNhLh5m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install gensim"],"metadata":{"id":"tVOd-PgoLK6v"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Notes:\n","\n","Restart the session and start executing only the step below. No need to execute the steps above."],"metadata":{"id":"Lpd2HM1UScc3"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import gensim"],"metadata":{"id":"qq6MzLm_Mzxj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pd.__version__"],"metadata":{"id":"C3Fh02O8M4mq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["np.__version__"],"metadata":{"id":"9beBLDk9M6Lr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["gensim.__version__"],"metadata":{"id":"Q0oNw16VM8Na"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from gensim.test.utils import datapath, get_tmpfile\n","from gensim.models import KeyedVectors\n","\n","word2vec_output_file = \"/content/glove/glove.6B.50d.txt\""],"metadata":{"id":"EQEBccoWG_ZE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pretrained_w2v_model = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False, no_header=True)"],"metadata":{"id":"CDSlnOYqHFXH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(pretrained_w2v_model)"],"metadata":{"id":"8MhTn6oVYLFz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pretrained_w2v_model.most_similar('bangalore')"],"metadata":{"id":"CzodvN1LHbo8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pretrained_w2v_model.most_similar('dhoni')"],"metadata":{"id":"c61PbcV0H1sY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pretrained_w2v_model.most_similar('google')"],"metadata":{"id":"0Ri2LbuZH7vW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pretrained_w2v_model.most_similar('hp')"],"metadata":{"id":"oL_867btJDIK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pretrained_w2v_model.most_similar('wikipedia')"],"metadata":{"id":"7CiiUxAnJGaa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def analogy(a, b, c):\n","    result = pretrained_w2v_model.most_similar([c, b], [a])\n","    return result[0][0]"],"metadata":{"id":"Re5m3yCpIDvW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["analogy('india', 'indian', 'japan')"],"metadata":{"id":"HN3-w5sMIIiu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["analogy('india', 'delhi', 'france')"],"metadata":{"id":"Vck-uW0WIVKL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["analogy('india', 'dhoni', 'england')"],"metadata":{"id":"pYVkNww-IdZp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"ZUywYW9tbNHA"},"source":["## Excellent References\n","\n","For further exploration and better understanding, you can use the following references.\n","\n","- Glossary of Deep Learning: Word Embedding\n","\n","    https://medium.com/deeper-learning/glossary-of-deep-learning-word-embedding-f90c3cec34ca\n","\n","\n","- wevi: word embedding visual inspector\n","\n","    https://ronxin.github.io/wevi/  \n","    \n","    \n","- Learning Word Embedding    \n","\n","    https://lilianweng.github.io/lil-log/2017/10/15/learning-word-embedding.html\n","\n","\n","- On the contribution of neural networks and word embeddings in Natural Language Processing\n","\n","    https://medium.com/@josecamachocollados/on-the-contribution-of-neural-networks-and-word-embeddings-in-natural-language-processing-c8bb1b85c61c"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"},"colab":{"provenance":[]},"gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}